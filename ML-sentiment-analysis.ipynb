{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Sistema de An√°lise de Sentimentos"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"pG9eugOTJYNO"},"outputs":[],"source":["# Importando bibliotecas\n","\n","import nltk, re, string\n","from nltk.corpus import stopwords, twitter_samples\n","import numpy as np\n","import pickle"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"CkaGbJoUJoOC"},"outputs":[],"source":["# Preprocessamento dos tweets\n","\n","def process_tweet(tweet):\n","    stemmer = nltk.PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    tweet = re.sub(r'#', '', tweet)\n","    tokenizer = nltk.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and\n","                word not in string.punctuation):\n","            stem_word = stemmer.stem(word)  # stemming word\n","            tweets_clean.append(stem_word)\n","\n","    return tweets_clean\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"aPTgml3SJ0pJ"},"outputs":[],"source":["# Esta √© a parte mais importante de todo o c√≥digo:\n","# A raz√£o √© que nosso conjunto de features, no qual treinaremos nosso modelo, ser√° constru√≠do aqui.\n","\n","def build_freqs(tweets, ys):\n","    \"\"\"Construir frequ√™ncias.\n","    Entrada:\n","        tweets: uma lista de tweets\n","        ys: uma matriz m x 1 com o r√≥tulo de sentimento de cada tweet\n","            (0 ou 1)\n","    Sa√≠da:\n","        freqs: um dicion√°rio que mapeia cada par (palavra, sentimento) para sua\n","        frequ√™ncia\n","    \"\"\"\n","    # Converter o array np em lista, pois o zip requer um iter√°vel.\n","    # O squeeze √© necess√°rio, sen√£o a lista ter√° um elemento.\n","    # Isso √© um NOP se 'ys' j√° for uma lista.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Come√ßar com um dicion√°rio vazio e preench√™-lo percorrendo todos os tweets\n","    # e todas as palavras processadas em cada tweet.\n","    freqs = {}\n","    for y, tweet in zip(yslist, tweets):\n","        for word in process_tweet(tweet):\n","            par = (word, y)\n","            if par in freqs:\n","                freqs[par] += 1\n","            else:\n","                freqs[par] = 1\n","\n","    return freqs\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":510,"status":"ok","timestamp":1622362224413,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"Y61jYX3fUkkU","outputId":"2fc1c5a3-791f-4cc6-d997-d61193ee40be"},"outputs":[{"name":"stdout","output_type":"stream","text":["{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}\n"]}],"source":["# Check de como o c√≥digo funciona\n","\n","tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n","ys = [1, 0, 0, 0, 0]\n","res = build_freqs(tweets, ys)\n","print(res)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"_aKmKS4-KJvp"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to C:\\Users\\Sepp-Kali-\n","[nltk_data]     Linux\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n","[nltk_data] Downloading package stopwords to C:\\Users\\Sepp-Kali-\n","[nltk_data]     Linux\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('twitter_samples')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"hxViCeI-KDN7"},"outputs":[],"source":["# sele√ß√£o dos conjuntos de tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"03NIPdv1KF96"},"outputs":[],"source":["# separando os dados em duas partes: treino e teste(valida√ß√£o)\n","test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"MOGG2-3WKRQ7"},"outputs":[],"source":["train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"dGQdgF5VKTia"},"outputs":[],"source":["# Combina√ß√£o de r√≥tulos positivos e negativos\n","# Estamos constru√≠ndo nossa vari√°vel 'y'-meta aqui\n","\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"QVDUs1paKb4z"},"outputs":[],"source":["# Cria√ß√£o do dicion√°rio de frequ√™ncia\n","freqs = build_freqs(train_x, train_y)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1622359584610,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"IV5_4qVUKewa","outputId":"2452a25e-60e3-4e98-81c8-3d3c7205f3a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 11337\n"]}],"source":["# Verifica√ß√£o de sa√≠da\n","\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":359,"status":"ok","timestamp":1622359619111,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"ZyVQ7YMoKp2T","outputId":"afcc5ef9-63f7-483c-ba67-ffbc682164a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Exemplo de um tweet positivo: \n"," üíÖüèΩüíã - :)))) haven't seen you in years\n","\n","Exemplo de vers√£o processada do tweet: \n"," ['üíÖüèΩ', 'üíã', ':)', 'seen', 'year']\n"]}],"source":["# Teste de fun√ß√£o\n","\n","print('Exemplo de um tweet positivo: \\n', train_x[24])\n","print('\\nExemplo de vers√£o processada do tweet: \\n', process_tweet(train_x[24]))"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"RCaQzncSKwL_"},"outputs":[],"source":["# Constru√ß√£o do modelo de Regress√£o Log√≠stica a partir do ZERO\n","\n","# Fun√ß√£o Sigmoide\n","def sigmoid(z):\n","    \"\"\"\n","    Entrada:\n","        z: √© a entrada (pode ser um escalar ou um array)\n","    Sa√≠da:\n","        h: a sigmoide de z\n","    \"\"\"\n","    zz = np.negative(z)\n","    h = 1 / (1 + np.exp(zz))\n","    return h\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ARg-_KLtK-ZX"},"outputs":[],"source":["# Fun√ß√£o de Descida de Gradiente e Custo\n","\n","def descidaGradiente(x, y, theta, alpha, num_iters):\n","    \"\"\"\n","    Entrada:\n","        x: matriz de features, que √© (m,n+1)\n","        y: r√≥tulos correspondentes da matriz de entrada x, dimens√µes (m,1)\n","        theta: vetor de pesos, dimens√£o (n+1,1)\n","        alpha: taxa de aprendizado\n","        num_iters: n√∫mero de itera√ß√µes para treinar o modelo\n","    Sa√≠da:\n","        J: o custo final\n","        theta: seu vetor de pesos final\n","    Dica: voc√™ pode imprimir o custo para garantir que esteja diminuindo.\n","    \"\"\"\n","    # obter 'm', o n√∫mero de linhas na matriz x\n","    m = x.shape[0]\n","    for i in range(0, num_iters):\n","        z = np.dot(x, theta)\n","        h = sigmoid(z)\n","        # calcular a fun√ß√£o de custo\n","        custo = -1. / m * (np.dot(y.transpose(), np.log(h)) + np.dot((1 - y).transpose(), np.log(1 - h)))\n","        # atualizar os pesos theta\n","        theta = theta - (alpha / m) * np.dot(x.transpose(), (h - y))\n","\n","    custo = float(custo)\n","    return custo, theta\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"i3RneTodLBcC"},"outputs":[],"source":["#  Extraindo os recursos\n","\n","def extract_features(tweet, freqs):\n","    \"\"\"\n","    Entrada:\n","        tweet: uma lista de palavras para um tweet\n","        freqs: um dicion√°rio correspondente √†s frequ√™ncias de cada tupla (palavra, r√≥tulo)\n","    Sa√≠da:\n","        x: um vetor de caracter√≠sticas de dimens√£o (1,3)\n","    \"\"\"\n","\n","    word_l = process_tweet(tweet)\n","    x = np.zeros((1, 3))\n","\n","    # o termo de vi√©s √© definido como 1\n","    x[0, 0] = 1\n","\n","    for word in word_l:\n","        # incrementar a contagem de palavras para o r√≥tulo positivo 1\n","        x[0, 1] += freqs.get((word, 1.0), 0)\n","        # incrementar a contagem de palavras para o r√≥tulo negativo 0\n","        x[0, 2] += freqs.get((word, 0.0), 0)\n","\n","    assert (x.shape == (1, 3))\n","    return x"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1622359716199,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"dSTFB_uCLEdz","outputId":"aa24fe51-efc3-421d-e039-31f622e188b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.000e+00 3.006e+03 1.240e+02]]\n"]}],"source":["# dados de treino e teste (valida√ß√£o)\n","\n","tmp1 = extract_features(train_x[22], freqs)\n","print(tmp1)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"RRFwSxLILIDI"},"outputs":[],"source":["# Esses tr√™s n√∫meros s√£o o conjunto de recursos que constru√≠mos usando as fun√ß√µes build_freq() e extract_features().\n","# build_freq() constr√≥i um dicion√°rio tendo palavras como chaves e o n√∫mero de vezes que elas ocorreram no corpus como valores.\n","# O recurso de extra√ß√£o leva em conta esses valores para palavras positivas e negativas, ou seja, tmp1[1] e tmp[2]\n"]},{"cell_type":"markdown","metadata":{"id":"eiHmtVaIN5KB"},"source":["### Como esses recursos ser√£o usados para previs√µes na regress√£o log√≠stica:\n","\n","- Primeiro √© constru√≠da uma hip√≥tese que para o nosso caso ser√° h(x) = b1 + b2*x1 + b3*x2\n","- aqui b1 = 1, b2 e b3 s√£o determinados pela fun√ß√£o de custo e gradiente, x1 e x2 s√£o o conjunto de recursos de palavras positivas e negativas."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"4lUutIScOWj7"},"outputs":[],"source":["# Treinamento do modelo\n","\n","# coleta as caracter√≠sticas 'x' e as empilha em uma matriz 'X'\n","X = np.zeros((len(train_x), 3))\n","for i in range(len(train_x)):\n","    X[i, :] = extract_features(train_x[i], freqs)\n","\n","# r√≥tulos de treinamento correspondentes a X\n","Y = train_y\n","\n","# Aplica a descida de gradiente\n","# esses valores s√£o predefinido\n","J, theta = descidaGradiente(X, Y, np.zeros((3, 1)), 1e-9, 1500)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"mUJ-lXxfOcvJ"},"outputs":[],"source":["def prever_tweet(tweet, freqs, theta):\n","    \"\"\"\n","    Entrada:\n","        tweet: uma string\n","        freqs: um dicion√°rio correspondente √†s frequ√™ncias de cada tupla (palavra, r√≥tulo)\n","        theta: vetor de pesos (3,1)\n","    Sa√≠da:\n","        y_pred: a probabilidade de um tweet ser positivo ou negativo\n","    \"\"\"\n","    # extrai as caracter√≠sticas do tweet e armazena em x\n","    x = extract_features(tweet, freqs)\n","    y_pred = sigmoid(np.dot(x, theta))\n","\n","    return y_pred\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"EDQP49HuOhjt"},"outputs":[],"source":["def testar_regressao_logistica(test_x, test_y, freqs, theta):\n","    \"\"\"\n","    Entrada:\n","        test_x: uma lista de tweets\n","        test_y: vetor (m, 1) com os r√≥tulos correspondentes para a lista de tweets\n","        freqs: um dicion√°rio com a frequ√™ncia de cada par (ou tupla)\n","        theta: vetor de pesos de dimens√£o (3, 1)\n","    Sa√≠da:\n","        precisao: (# de tweets classificados corretamente) / (# total de tweets)\n","    \"\"\"\n","    # lista para armazenar as previs√µes\n","    y_hat = []\n","\n","    for tweet in test_x:\n","        # obter a previs√£o de r√≥tulo para o tweet\n","        y_pred = prever_tweet(tweet, freqs, theta)\n","        if y_pred > 0.5:\n","            y_hat.append(1)\n","        else:\n","            y_hat.append(0)\n","\n","    accuracy = (y_hat == np.squeeze(test_y)).sum() / len(test_x)\n","\n","    return accuracy\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1316,"status":"ok","timestamp":1622361634031,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"9-B599jISbFd","outputId":"45f6d6e3-4303-4004-9a43-549f5dc803d8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Precis√£o do modelo de Regress√£o Log√≠stica = 0.9950\n"]}],"source":["tmp_accuracy = testar_regressao_logistica(test_x, test_y, freqs, theta)\n","print(f\"Precis√£o do modelo de Regress√£o Log√≠stica = {tmp_accuracy:.4f}\")"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"w1RuRdapSdYB"},"outputs":[],"source":["# Fazendo 'Predict' com o pr√≥prio Tweet para teste\n","\n","def pre(sentence):\n","    yhat = prever_tweet(sentence, freqs, theta)\n","    if yhat > 0.5:\n","        return 'Sentimento: Positivo'\n","    elif yhat == 0:\n","        return 'Sentimento: Neutro'\n","    else:\n","        return 'Sentimento: Negativo'"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":360,"status":"ok","timestamp":1622361716855,"user":{"displayName":"siddharth patra","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjnwsdj5DKa95WHRl25dlQz9L1Bbc10vap4xTNIiA=s64","userId":"01966341811263607507"},"user_tz":-330},"id":"UfaCSbBzSoG7","outputId":"3f476d64-566b-4041-b67f-27c178c8e30a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentimento: Positivo\n"]}],"source":["my_tweet = 'I love this song'\n","\n","res = pre(my_tweet)\n","print(res)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"9LdyAf8mSs0P"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentimento: Negativo\n"]}],"source":["my_tweet = 'I am sad'\n","\n","res = pre(my_tweet)\n","print(res)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPhCtoyBwrvd2ZHTmIihM2m","collapsed_sections":[],"name":"Sentiment LG.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
